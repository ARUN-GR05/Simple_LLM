# ğŸš€ Simple LLM - Deployed!

A simple transformer-based language model deployed on Vercel.

## ğŸ“ Project Structure

```
Simple_LLM/
â”œâ”€â”€ backend/              # Python FastAPI server
â”‚   â”œâ”€â”€ app.py           # API endpoints
â”‚   â”œâ”€â”€ model.py         # LLM model & tokenizer
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ Dockerfile       # For local Docker testing
â”œâ”€â”€ frontend/            # Next.js React app
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx     # Main UI
â”‚   â”‚   â”œâ”€â”€ layout.tsx   # Layout
â”‚   â”‚   â””â”€â”€ globals.css  # Tailwind styles
â”‚   â””â”€â”€ package.json     # Node dependencies
â”œâ”€â”€ vercel.json          # Deployment config
â””â”€â”€ simple_LLM.ipynb    # Original notebook
```

## ğŸ› ï¸ Setup Instructions

### 1. **Install Dependencies**

**Frontend:**
```bash
cd frontend
npm install
```

**Backend (optional - for local testing):**
```bash
cd backend
pip install -r requirements.txt
```

### 2. **Environment Variables**

Create `frontend/.env.local`:
```
NEXT_PUBLIC_API_URL=https://your-backend-url.com
```

For local development:
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### 3. **Run Locally**

**Terminal 1 - Backend:**
```bash
cd backend
python app.py
# Runs on http://localhost:8000
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
# Runs on http://localhost:3000
```

## ğŸ“¤ Deploy to Vercel

### **Step 1: Create Backend (Flask/FastAPI on Render/Railway)**

Choose one platform:

**Option A: Deploy on Render (FREE)**
1. Go to [render.com](https://render.com)
2. Click "New" â†’ "Web Service"
3. Connect your GitHub repo
4. Set build command: `cd backend && pip install -r requirements.txt`
5. Set start command: `cd backend && python app.py`
6. Deploy and copy the URL

**Option B: Deploy on Railway (FREE with card)**
1. Go to [railway.app](https://railway.app)
2. Create new project â†’ Deploy from GitHub
3. Select the backend folder
4. Set PORT to 8000
5. Deploy

### **Step 2: Deploy Frontend on Vercel**

1. Go to [vercel.com](https://vercel.com)
2. Click "Add New" â†’ "Project"
3. Import your GitHub repo
4. **Framework: Next.js**
5. **Root Directory: frontend**
6. Add Environment Variable:
   - Key: `NEXT_PUBLIC_API_URL`
   - Value: `https://your-backend.onrender.com` (or your backend URL)
7. Click "Deploy"

## ğŸ”Œ API Endpoints

- `GET /` - Health check
- `GET /health` - Status
- `POST /generate` - Generate text
  ```json
  {
    "prompt": "The cat",
    "max_length": 20,
    "temperature": 0.7
  }
  ```

## ğŸ¨ Features

- âœ… Real-time text generation
- âœ… Adjustable temperature (randomness)
- âœ… Max length control
- âœ… Beautiful UI with Tailwind CSS
- âœ… Responsive design
- âœ… Error handling

## ğŸ“‹ Required Dependencies

**Frontend:**
- Next.js 14
- React 18
- Tailwind CSS
- Axios

**Backend:**
- FastAPI
- PyTorch
- Pydantic
- Uvicorn

## ğŸŒ Live Demo

Frontend: `https://your-app.vercel.app`
Backend API: `https://your-backend.onrender.com`

## ğŸ“ Notes

- The model runs on CPU (suitable for low-traffic apps)
- For production: Consider using GPU runtime or model quantization
- Temperature: 0.1 (deterministic) â†’ 2.0 (creative)
- Max length: 5-50 tokens recommended

## ğŸ› Troubleshooting

**CORS Error?**
â†’ Already handled in `backend/app.py` with `CORSMiddleware`

**Connection refused?**
â†’ Check `NEXT_PUBLIC_API_URL` in `.env.local`

**Model not found?**
â†’ Backend initializes model on startup automatically

## ğŸ“š Learn More

- [PyTorch Docs](https://pytorch.org)
- [FastAPI Docs](https://fastapi.tiangolo.com)
- [Next.js Docs](https://nextjs.org)
- [Vercel Deployment](https://vercel.com/docs)

---

**Ready to deploy?** Push to GitHub and follow the deployment steps above! ğŸ‰
